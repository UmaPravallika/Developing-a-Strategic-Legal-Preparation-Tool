{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Developing a Strategic Legal Preparation Tool :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Approach of Project* :\n",
    "\n",
    "1. #### *Collect Data*: Obtain PDF of the legal document.\n",
    "2. #### *Structure Document*: Split into {page, line, text}.\n",
    "3. #### *Keyword Extraction*: Use TextRank / KeyBERT / TF-IDF to get important                                      keywords.\n",
    "4. #### *Sentence Scoring*: Rank sentences based on keyword presence and importance.\n",
    "5. #### *Argument Classification*: Tag sentences as For or Against.\n",
    "6. #### *Reference Mapping: Attach* page and line numbers to each key sentence.\n",
    "7. #### *Top 10 Selection*: Pick the most pivotal items, ensuring balanced perspective.\n",
    "8. #### *Optional Summarization*: Abstractive summary for readability (keep original                                     sentences for reference).\n",
    "9. #### *Output*: Present in table/JSON with keyword, sentence, page, line, and stance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:57:38.721886Z",
     "iopub.status.busy": "2025-09-25T07:57:38.721498Z",
     "iopub.status.idle": "2025-09-25T07:57:45.102804Z",
     "shell.execute_reply": "2025-09-25T07:57:45.101612Z",
     "shell.execute_reply.started": "2025-09-25T07:57:38.721863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Obtaining dependency information for pdfplumber from https://files.pythonhosted.org/packages/db/e0/52b67d4f00e09e497aec4f71bc44d395605e8ebcea52543242ed34c25ef9/pdfplumber-0.11.7-py3-none-any.whl.metadata\n",
      "  Using cached pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Obtaining dependency information for pdfminer.six==20250506 from https://files.pythonhosted.org/packages/73/16/7a432c0101fa87457e75cb12c879e1749c5870a786525e2e0f42871d6462/pdfminer_six-20250506-py3-none-any.whl.metadata\n",
      "  Using cached pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/uma/anaconda3/lib/python3.11/site-packages (from pdfplumber) (10.2.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Obtaining dependency information for pypdfium2>=4.18.0 from https://files.pythonhosted.org/packages/65/cd/3f1edf20a0ef4a212a5e20a5900e64942c5a374473671ac0780eaa08ea80/pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/uma/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20250506->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/uma/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20250506->pdfplumber) (41.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/uma/anaconda3/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/uma/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Using cached pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/5.6 MB\u001b[0m \u001b[31m54.4 kB/s\u001b[0m eta \u001b[36m0:01:25\u001b[0m:20\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:57:45.104619Z",
     "iopub.status.busy": "2025-09-25T07:57:45.104318Z",
     "iopub.status.idle": "2025-09-25T07:59:09.207850Z",
     "shell.execute_reply": "2025-09-25T07:59:09.206616Z",
     "shell.execute_reply.started": "2025-09-25T07:57:45.104590Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install keybert sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:09.209519Z",
     "iopub.status.busy": "2025-09-25T07:59:09.209192Z",
     "iopub.status.idle": "2025-09-25T07:59:13.246205Z",
     "shell.execute_reply": "2025-09-25T07:59:13.245272Z",
     "shell.execute_reply.started": "2025-09-25T07:59:09.209485Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Import libraries  :*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:13.247775Z",
     "iopub.status.busy": "2025-09-25T07:59:13.247370Z",
     "iopub.status.idle": "2025-09-25T07:59:51.193505Z",
     "shell.execute_reply": "2025-09-25T07:59:51.192578Z",
     "shell.execute_reply.started": "2025-09-25T07:59:13.247734Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "import json                \n",
    "from tabulate import tabulate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from case :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:51.196897Z",
     "iopub.status.busy": "2025-09-25T07:59:51.196269Z",
     "iopub.status.idle": "2025-09-25T07:59:51.200748Z",
     "shell.execute_reply": "2025-09-25T07:59:51.200048Z",
     "shell.execute_reply.started": "2025-09-25T07:59:51.196874Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path=\"http://localhost:8888/files/Assignment/Moto-AI/Alliance-Hippocratic-Medicine_2023.02.13_AMICUS-BRIEF-State-of-Mississippi-et-al.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:51.201828Z",
     "iopub.status.busy": "2025-09-25T07:59:51.201510Z",
     "iopub.status.idle": "2025-09-25T07:59:53.086185Z",
     "shell.execute_reply": "2025-09-25T07:59:53.085349Z",
     "shell.execute_reply.started": "2025-09-25T07:59:51.201800Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    for page_num, page in enumerate(pdf.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            lines = text.split('\\n')\n",
    "            for line_num, line in enumerate(lines, start=1):\n",
    "                all_data.append({\n",
    "                    \"Page\": page_num,  # store page number\n",
    "                    \"Line\": line_num,  # line number\n",
    "                    \"Text\": line       # store single line\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data into Dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:53.087603Z",
     "iopub.status.busy": "2025-09-25T07:59:53.087213Z",
     "iopub.status.idle": "2025-09-25T07:59:53.096283Z",
     "shell.execute_reply": "2025-09-25T07:59:53.095535Z",
     "shell.execute_reply.started": "2025-09-25T07:59:53.087568Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:08:18.094282Z",
     "iopub.status.busy": "2025-09-25T08:08:18.093916Z",
     "iopub.status.idle": "2025-09-25T08:08:18.104743Z",
     "shell.execute_reply": "2025-09-25T08:08:18.103803Z",
     "shell.execute_reply.started": "2025-09-25T08:08:18.094257Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords extraction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:53.135547Z",
     "iopub.status.busy": "2025-09-25T07:59:53.135198Z",
     "iopub.status.idle": "2025-09-25T07:59:53.140936Z",
     "shell.execute_reply": "2025-09-25T07:59:53.140081Z",
     "shell.execute_reply.started": "2025-09-25T07:59:53.135519Z"
    }
   },
   "outputs": [],
   "source": [
    "full_text=\" \".join(df[\"Text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T07:59:53.142228Z",
     "iopub.status.busy": "2025-09-25T07:59:53.141769Z",
     "iopub.status.idle": "2025-09-25T08:00:01.889271Z",
     "shell.execute_reply": "2025-09-25T08:00:01.888391Z",
     "shell.execute_reply.started": "2025-09-25T07:59:53.142197Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_model=KeyBERT()\n",
    "def extract_keywords(sent):\n",
    "    kws = kw_model.extract_keywords(sent, keyphrase_ngram_range=(1,3), top_n=5)\n",
    "    return [kw[0] for kw in kws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:07:03.092626Z",
     "iopub.status.busy": "2025-09-25T08:07:03.092205Z",
     "iopub.status.idle": "2025-09-25T08:07:16.611371Z",
     "shell.execute_reply": "2025-09-25T08:07:16.610437Z",
     "shell.execute_reply.started": "2025-09-25T08:07:03.092599Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords=extract_keywords(full_text)\n",
    "print(\"Keywords extracted : \\n\",keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:15.225275Z",
     "iopub.status.busy": "2025-09-25T08:00:15.224729Z",
     "iopub.status.idle": "2025-09-25T08:00:16.322531Z",
     "shell.execute_reply": "2025-09-25T08:00:16.321671Z",
     "shell.execute_reply.started": "2025-09-25T08:00:15.225232Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(full_text)\n",
    "sentence_data = []\n",
    "\n",
    "for sent in sentences:\n",
    "    sent_clean = clean_text(sent)\n",
    "    matched = False\n",
    "    for idx, row in df.iterrows():\n",
    "        line_clean = clean_text(row[\"Text\"])\n",
    "        if sent_clean in line_clean or line_clean in sent_clean:\n",
    "            sentence_data.append({\n",
    "                \"Sentence \": sent_clean,\n",
    "                \"Page \": row[\"Page\"],\n",
    "                \"Line \": row[\"Line\"]\n",
    "            })\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        sentence_data.append({\n",
    "            \"Sentence \": sent_clean,\n",
    "            \"Page \": None,\n",
    "            \"Line \": None\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sentence_data into Dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.323757Z",
     "iopub.status.busy": "2025-09-25T08:00:16.323495Z",
     "iopub.status.idle": "2025-09-25T08:00:16.328813Z",
     "shell.execute_reply": "2025-09-25T08:00:16.327930Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.323737Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_df=pd.DataFrame(sentence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence scoring based on Keywords :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.330111Z",
     "iopub.status.busy": "2025-09-25T08:00:16.329788Z",
     "iopub.status.idle": "2025-09-25T08:00:16.347945Z",
     "shell.execute_reply": "2025-09-25T08:00:16.346877Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.330085Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword_list=[kw[0].lower() for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.349264Z",
     "iopub.status.busy": "2025-09-25T08:00:16.349001Z",
     "iopub.status.idle": "2025-09-25T08:00:16.367537Z",
     "shell.execute_reply": "2025-09-25T08:00:16.366481Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.349243Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_sentences(sent):\n",
    "    score=0\n",
    "    for key in keyword_list:\n",
    "        if key in sent :\n",
    "            score+=1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.368795Z",
     "iopub.status.busy": "2025-09-25T08:00:16.368536Z",
     "iopub.status.idle": "2025-09-25T08:00:16.393027Z",
     "shell.execute_reply": "2025-09-25T08:00:16.391879Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.368775Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_df[\"Score\"]=sent_df[\"Sentence \"].apply(score_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.394426Z",
     "iopub.status.busy": "2025-09-25T08:00:16.394108Z",
     "iopub.status.idle": "2025-09-25T08:00:16.446475Z",
     "shell.execute_reply": "2025-09-25T08:00:16.445167Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.394393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting in ascending order :\n",
    "sent_df=sent_df.sort_values(by=\"Score\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Classification : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.448313Z",
     "iopub.status.busy": "2025-09-25T08:00:16.447745Z",
     "iopub.status.idle": "2025-09-25T08:00:16.501195Z",
     "shell.execute_reply": "2025-09-25T08:00:16.499959Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.448280Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def argument_classification(sent):\n",
    "    for_words = [\n",
    "        \"violate\", \"defy\", \"unlawful\", \"undermine\", \"harm\", \"oppose\", \"restrict\",\n",
    "        \"protect\", \"safeguard\", \"defiance\", \"illegal\", \"contravene\", \"obstruction\",\n",
    "        \"prohibited\", \"threaten\", \"contrary\", \"encroachment\", \"evade\", \"disregard\",\n",
    "        \"jeopardize\", \"risk\", \"impose\", \"conflict\", \"unlawfulness\", \"infringement\",\n",
    "        \"outweigh\", \"undermine public interest\"\n",
    "    ]\n",
    "    \n",
    "    against_words = [\n",
    "        \"access\", \"available\", \"expand\", \"promote\", \"ensure\", \"safely\", \"permitted\",\n",
    "        \"authorized\", \"approved\", \"provide\", \"therapeutic benefit\", \"enforce discretion\",\n",
    "        \"facilitate\"\n",
    "    ]\n",
    "\n",
    "    # Lowercase and remove punctuation\n",
    "    sent_clean = sent.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Check for 'for' phrases/words\n",
    "    for phrase in for_words:\n",
    "        if phrase in sent_clean:\n",
    "            return \"For\"\n",
    "\n",
    "    # Check for 'against' phrases/words\n",
    "    for phrase in against_words:\n",
    "        if phrase in sent_clean:\n",
    "            return \"Against\"\n",
    "\n",
    "    return \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.502333Z",
     "iopub.status.busy": "2025-09-25T08:00:16.502000Z",
     "iopub.status.idle": "2025-09-25T08:00:16.567215Z",
     "shell.execute_reply": "2025-09-25T08:00:16.566121Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.502307Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_df[\"Argument\"]=sent_df[\"Sentence \"].apply(argument_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.571409Z",
     "iopub.status.busy": "2025-09-25T08:00:16.571148Z",
     "iopub.status.idle": "2025-09-25T08:00:16.589591Z",
     "shell.execute_reply": "2025-09-25T08:00:16.588742Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.571390Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_df[\"Argument\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:07:47.330086Z",
     "iopub.status.busy": "2025-09-25T08:07:47.329753Z",
     "iopub.status.idle": "2025-09-25T08:07:47.338329Z",
     "shell.execute_reply": "2025-09-25T08:07:47.337440Z",
     "shell.execute_reply.started": "2025-09-25T08:07:47.330055Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sent_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Pivotal sentences : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.608316Z",
     "iopub.status.busy": "2025-09-25T08:00:16.608054Z",
     "iopub.status.idle": "2025-09-25T08:00:16.630828Z",
     "shell.execute_reply": "2025-09-25T08:00:16.629959Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.608295Z"
    }
   },
   "outputs": [],
   "source": [
    "for_sentences=sent_df[sent_df[\"Argument\"]==\"For\"].head(5)\n",
    "against_sentences=sent_df[sent_df[\"Argument\"]==\"Against\"].head(5)\n",
    "\n",
    "top_10=pd.concat([for_sentences,against_sentences])\n",
    "top_10=top_10[[\"Sentence \",\"Page \",\"Line \",\"Argument\",\"Score\"]]\n",
    "\n",
    "print(\"Top 10 sentences : \\n\",top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summarization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.631917Z",
     "iopub.status.busy": "2025-09-25T08:00:16.631672Z",
     "iopub.status.idle": "2025-09-25T08:00:16.648183Z",
     "shell.execute_reply": "2025-09-25T08:00:16.647302Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.631897Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_for_summary(text):\n",
    "    # Remove case numbers\n",
    "    text = re.sub(r'Case \\d+:[\\d\\-]+ Document \\d+ Filed \\d+/\\d+/\\d+', '', text)\n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    # Remove all-uppercase words (likely headers or states)\n",
    "    text = \" \".join([line for line in text.split('.') if not line.isupper()])\n",
    "    # Remove extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.649660Z",
     "iopub.status.busy": "2025-09-25T08:00:16.649289Z",
     "iopub.status.idle": "2025-09-25T08:00:16.667875Z",
     "shell.execute_reply": "2025-09-25T08:00:16.666889Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.649638Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_text = clean_text_for_summary(full_text)\n",
    "input_text = clean_text  # keep within model limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:16.669253Z",
     "iopub.status.busy": "2025-09-25T08:00:16.668929Z",
     "iopub.status.idle": "2025-09-25T08:00:22.467422Z",
     "shell.execute_reply": "2025-09-25T08:00:22.466608Z",
     "shell.execute_reply.started": "2025-09-25T08:00:16.669222Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def summarize_text(text, max_input_len=512, max_output_len=150):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=max_input_len)\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_output_len, \n",
    "        min_length=40, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Summarize only the top 10 sentences\n",
    "top_text = \" \".join(top_10[\"Sentence \"].tolist())\n",
    "summary_text = summarize_text(top_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:22.468625Z",
     "iopub.status.busy": "2025-09-25T08:00:22.468368Z",
     "iopub.status.idle": "2025-09-25T08:00:25.306290Z",
     "shell.execute_reply": "2025-09-25T08:00:25.305317Z",
     "shell.execute_reply.started": "2025-09-25T08:00:22.468605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take top 50 scored sentences\n",
    "top_sentences = sent_df.head(10)[\"Sentence \"].tolist()\n",
    "summary_input = \" \".join(top_sentences)\n",
    "\n",
    "# Summarize using T5\n",
    "summary_text = summarize_text(summary_input, max_input_len=512, max_output_len=500)\n",
    "print(\"Top 10 sentences Abstractive Summary:\\n\", summary_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting into JSON Format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:25.307713Z",
     "iopub.status.busy": "2025-09-25T08:00:25.307406Z",
     "iopub.status.idle": "2025-09-25T08:00:25.317528Z",
     "shell.execute_reply": "2025-09-25T08:00:25.316483Z",
     "shell.execute_reply.started": "2025-09-25T08:00:25.307681Z"
    }
   },
   "outputs": [],
   "source": [
    "top_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:25.318878Z",
     "iopub.status.busy": "2025-09-25T08:00:25.318595Z",
     "iopub.status.idle": "2025-09-25T08:00:26.571572Z",
     "shell.execute_reply": "2025-09-25T08:00:26.570324Z",
     "shell.execute_reply.started": "2025-09-25T08:00:25.318859Z"
    }
   },
   "outputs": [],
   "source": [
    "top_10[\"Keywords\"] = top_10[\"Sentence \"].apply(extract_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:26.572751Z",
     "iopub.status.busy": "2025-09-25T08:00:26.572525Z",
     "iopub.status.idle": "2025-09-25T08:00:26.578244Z",
     "shell.execute_reply": "2025-09-25T08:00:26.577308Z",
     "shell.execute_reply.started": "2025-09-25T08:00:26.572735Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_df[\"Page \"] = sent_df[\"Page \"].apply(lambda x: x.page_number if hasattr(x, \"page_number\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:00:26.579576Z",
     "iopub.status.busy": "2025-09-25T08:00:26.579245Z",
     "iopub.status.idle": "2025-09-25T08:00:26.603317Z",
     "shell.execute_reply": "2025-09-25T08:00:26.602294Z",
     "shell.execute_reply.started": "2025-09-25T08:00:26.579546Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output = {\n",
    "    \"Top_10_Sentences\": top_10.to_dict(orient=\"records\"),\n",
    "    \"Summary\": summary_text\n",
    "}\n",
    "\n",
    "# Pretty print\n",
    "print(json.dumps(output, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:03:39.754432Z",
     "iopub.status.busy": "2025-09-25T08:03:39.752934Z",
     "iopub.status.idle": "2025-09-25T08:03:40.340095Z",
     "shell.execute_reply": "2025-09-25T08:03:40.339286Z",
     "shell.execute_reply.started": "2025-09-25T08:03:39.754387Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df_vis = pd.DataFrame(output[\"Top_10_Sentences\"])\n",
    "sns.countplot(x=\"Argument\", data=df_vis, palette=\"Set2\")\n",
    "plt.title(\"Distribution of Arguments (For vs Against)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:04:01.291653Z",
     "iopub.status.busy": "2025-09-25T08:04:01.290707Z",
     "iopub.status.idle": "2025-09-25T08:04:01.906984Z",
     "shell.execute_reply": "2025-09-25T08:04:01.905729Z",
     "shell.execute_reply.started": "2025-09-25T08:04:01.291624Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "keywords = [kw for item in df_vis[\"Keywords\"] for kw in item]\n",
    "word_freq = Counter(keywords)\n",
    "\n",
    "wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Keyword Importance (WordCloud)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:04:13.567436Z",
     "iopub.status.busy": "2025-09-25T08:04:13.566631Z",
     "iopub.status.idle": "2025-09-25T08:04:14.016454Z",
     "shell.execute_reply": "2025-09-25T08:04:14.015404Z",
     "shell.execute_reply.started": "2025-09-25T08:04:13.567406Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Score\", y=\"Sentence \", data=df_vis, palette=\"Blues_d\")\n",
    "plt.title(\"Top 10 Sentences by Score\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Sentence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T08:04:37.991069Z",
     "iopub.status.busy": "2025-09-25T08:04:37.990717Z",
     "iopub.status.idle": "2025-09-25T08:04:38.187510Z",
     "shell.execute_reply": "2025-09-25T08:04:38.186333Z",
     "shell.execute_reply.started": "2025-09-25T08:04:37.991039Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_vis[\"Page \"].dropna(), bins=10, kde=False)\n",
    "plt.title(\"Distribution of Key Sentences Across Pages\")\n",
    "plt.xlabel(\"Page Number\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not available\n",
    "# !pip install python-pptx\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "# Create presentation\n",
    "prs = Presentation()\n",
    "title_slide_layout = prs.slide_layouts[0]\n",
    "\n",
    "# --- Slide 1: Title ---\n",
    "slide = prs.slides.add_slide(title_slide_layout)\n",
    "title = slide.shapes.title\n",
    "subtitle = slide.placeholders[1]\n",
    "title.text = \"Developing a Strategic Legal Preparation Tool\"\n",
    "subtitle.text = \"AI & Machine Learning Project\\nPrepared by: Uma Pravallika\"\n",
    "\n",
    "# --- Slide 2: Problem Statement ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Problem Statement\"\n",
    "content.text = (\n",
    "    \"In legal proceedings, attorneys need to analyze lengthy documents quickly.\\n\"\n",
    "    \"Our task: Develop an AI tool that extracts the Top 10 pivotal arguments (For & Against), \"\n",
    "    \"with references to page and line numbers, to help attorneys prepare effectively.\"\n",
    ")\n",
    "\n",
    "# --- Slide 3: Approach Overview ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Approach of the Project\"\n",
    "content.text = (\n",
    "    \"1. Collect Data: Extract text from PDF using pdfplumber.\\n\"\n",
    "    \"2. Structure Document: Split into {page, line, text}.\\n\"\n",
    "    \"3. Keyword Extraction: Use KeyBERT / TF-IDF.\\n\"\n",
    "    \"4. Sentence Scoring: Rank sentences based on keywords.\\n\"\n",
    "    \"5. Argument Classification: Tag as For / Against.\\n\"\n",
    "    \"6. Reference Mapping: Attach page & line numbers.\\n\"\n",
    "    \"7. Top 10 Selection: Ensure balanced arguments.\\n\"\n",
    "    \"8. Summarization: Generate abstractive summary with T5.\\n\"\n",
    "    \"9. Output: Present in JSON/Table format.\"\n",
    ")\n",
    "\n",
    "# --- Slide 4: Data Processing ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Data Processing\"\n",
    "content.text = (\n",
    "    \"- Extracted text using pdfplumber.\\n\"\n",
    "    \"- Structured into rows with Page, Line, Text.\\n\"\n",
    "    \"- Applied cleaning & tokenization using NLTK.\\n\"\n",
    "    \"- Prepared text for keyword extraction and scoring.\"\n",
    ")\n",
    "\n",
    "# --- Slide 5: Keyword Extraction ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Keyword Extraction\"\n",
    "content.text = (\n",
    "    \"- Applied KeyBERT to extract top keywords.\\n\"\n",
    "    \"- Keywords highlight critical legal terms.\\n\"\n",
    "    \"- Used TF-IDF for scoring importance.\"\n",
    ")\n",
    "\n",
    "# --- Slide 6: Sentence Scoring & Classification ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Sentence Scoring & Classification\"\n",
    "content.text = (\n",
    "    \"- Scored sentences based on keyword frequency.\\n\"\n",
    "    \"- Classified as 'For' or 'Against' using rule-based matching.\\n\"\n",
    "    \"- Ensured balanced selection of 5 'For' and 5 'Against'.\"\n",
    ")\n",
    "\n",
    "# --- Slide 7: Top 10 Sentences Example ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Top 10 Key Sentences\"\n",
    "content.text = (\n",
    "    \"Example Output:\\n\"\n",
    "    \"• 'Under our Constitution, States have the primary authority to legislate...' (For)\\n\"\n",
    "    \"• 'In 2016, the FDA extended the approved use of mifepristone...' (Against)\\n\"\n",
    "    \"• Each entry includes Page & Line references.\"\n",
    ")\n",
    "\n",
    "# --- Slide 8: Abstractive Summary ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Abstractive Summary\"\n",
    "content.text = (\n",
    "    \"Generated summary using T5 model:\\n\"\n",
    "    \"“The agency relied on Subpart H when it first approved mifepristone in 2000. \"\n",
    "    \"It required supervision by physicians, reflecting states' power to regulate healthcare.”\"\n",
    ")\n",
    "\n",
    "# --- Slide 9: Output Format ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Final Output\"\n",
    "content.text = (\n",
    "    \"- JSON file with Top 10 sentences, arguments, keywords.\\n\"\n",
    "    \"- Table format for readability.\\n\"\n",
    "    \"- Includes page & line references for quick lookup.\"\n",
    ")\n",
    "\n",
    "# --- Slide 10: Screenshots & Visuals ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[5])  # Title only\n",
    "title = slide.shapes.title\n",
    "title.text = \"Screenshots of Outputs\"\n",
    "# (You can manually insert images from Outputs.zip here after generating ppt)\n",
    "\n",
    "# --- Slide 11: Conclusion & Future Scope ---\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title, content = slide.shapes.title, slide.placeholders[1]\n",
    "title.text = \"Conclusion & Future Scope\"\n",
    "content.text = (\n",
    "    \"- Automated legal document summarization saves time.\\n\"\n",
    "    \"- Helps attorneys focus on key case aspects.\\n\"\n",
    "    \"- Future scope: Use GPT/LLMs for deeper contextual analysis, \"\n",
    "    \"multi-document summarization, and interactive dashboards.\"\n",
    ")\n",
    "\n",
    "# Save presentation\n",
    "prs.save(\"Strategic_Legal_Preparation_Tool_Final.pptx\")\n",
    "print(\"✅ PPT Generated: Strategic_Legal_Preparation_Tool_Final.pptx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8341604,
     "sourceId": 13164693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
